{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold, GridSearchCV, cross_val_score, cross_val_predict, validation_curve\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from numpy import nan\n",
    "%matplotlib inline\n",
    "from sklearn import metrics\n",
    "plt.rcParams['figure.figsize'] = (30,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#description, scope of project\n",
    "Start by importing the dataset and assigning column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32561, 15)\n",
      "(16281, 15)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('adult.csv', header=None, names=[\"age\", \"wclass\", \"fnlwgt\", \"education\", \"education-num\", \"mstatus\", \\\n",
    "                                                 \"occ\", \"relationship\", \"race\", \"sex\", \"capgain\", \"caploss\", \\\n",
    "                                                  \"hperweek\", \"country\", \"class\"])\n",
    "df_test = pd.read_csv('adult.test.csv', header=None, names=[\"age\", \"wclass\", \"fnlwgt\", \"education\", \"education-num\", \"mstatus\", \\\n",
    "                                                 \"occ\", \"relationship\", \"race\", \"sex\", \"capgain\", \"caploss\", \\\n",
    "                                                  \"hperweek\", \"country\", \"class\"])\n",
    "print df_train.shape\n",
    "print df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to check for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age              0\n",
       "wclass           0\n",
       "fnlwgt           0\n",
       "education        0\n",
       "education-num    0\n",
       "mstatus          0\n",
       "occ              0\n",
       "relationship     0\n",
       "race             0\n",
       "sex              0\n",
       "capgain          0\n",
       "caploss          0\n",
       "hperweek         0\n",
       "country          0\n",
       "class            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values could be there in other ways. One of the most common ways is '?'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cat adult.csv | grep '?' | wc -l\n",
    "#cat adult.test.csv | grep '?' | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the output as 2399 and 1221, which are exactly the number of missing values given at http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names. Hence these are the only missing values. We replace them with nan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                 0\n",
      "wclass           1836\n",
      "fnlwgt              0\n",
      "education           0\n",
      "education-num       0\n",
      "mstatus             0\n",
      "occ              1843\n",
      "relationship        0\n",
      "race                0\n",
      "sex                 0\n",
      "capgain             0\n",
      "caploss             0\n",
      "hperweek            0\n",
      "country           583\n",
      "class               0\n",
      "dtype: int64\n",
      "\n",
      "age                0\n",
      "wclass           963\n",
      "fnlwgt             0\n",
      "education          0\n",
      "education-num      0\n",
      "mstatus            0\n",
      "occ              966\n",
      "relationship       0\n",
      "race               0\n",
      "sex                0\n",
      "capgain            0\n",
      "caploss            0\n",
      "hperweek           0\n",
      "country          274\n",
      "class              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train.replace({' ?':nan})\n",
    "print df_train.isnull().sum()\n",
    "df_test = df_test.replace({' ?':nan})\n",
    "print \"\\n\", df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To figure out how to replace missing values, lets look at distribution of values of each \"missing value\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Private             22696\n",
      " Self-emp-not-inc     2541\n",
      " Local-gov            2093\n",
      "Name: wclass, dtype: int64\n",
      "\n",
      " Prof-specialty     4140\n",
      " Craft-repair       4099\n",
      " Exec-managerial    4066\n",
      "Name: occ, dtype: int64\n",
      "\n",
      " United-States    29170\n",
      " Mexico             643\n",
      " Philippines        198\n",
      "Name: country, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print df_train['wclass'].value_counts().head(3)\n",
    "print \"\\n\", df_train['occ'].value_counts().head(3)\n",
    "print \"\\n\", df_train['country'].value_counts().head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For wclass and country, it seems ok to replace it by the mode - since the probability of that value being the mode is high. (Ideally, if it is greater than our accuracy, we should be good). 'Private' is 70% of training, country is '90%'. We can also look at how much each variable is contributing to the separability of the classes by plotting a scatter matrix. For now, avoid premature optimization, we'll just replace it by mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = ['wclass','occ','country']\n",
    "df_train[cols] = df_train[cols].fillna(df_train.mode().iloc[0])\n",
    "df_test[cols] = df_test[cols].fillna(df_train.mode().iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll transform the categorical features to numerical. The features which need to be transformed are 'wclass', 'mstatus', 'occ', 'relationship', 'race', 'sex', 'country', 'education' and 'class'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   wclass  mstatus  occ  relationship  race  sex  country  education  class\n",
      "0     6.0      4.0  0.0           1.0   4.0  1.0     38.0        9.0    0.0\n",
      "1     5.0      2.0  3.0           0.0   4.0  1.0     38.0        9.0    0.0\n",
      "2     3.0      0.0  5.0           1.0   4.0  1.0     38.0       11.0    0.0\n",
      "3     3.0      2.0  5.0           0.0   2.0  1.0     38.0        1.0    0.0\n",
      "4     3.0      2.0  9.0           5.0   2.0  0.0      4.0        9.0    0.0\n",
      "   wclass  mstatus   occ  relationship  race  sex  country  education  class\n",
      "0     3.0      4.0   6.0           3.0   2.0  1.0     38.0        1.0    0.0\n",
      "1     3.0      2.0   4.0           0.0   4.0  1.0     38.0       11.0    0.0\n",
      "2     1.0      2.0  10.0           0.0   4.0  1.0     38.0        7.0    1.0\n",
      "3     3.0      2.0   6.0           0.0   2.0  1.0     38.0       15.0    1.0\n",
      "4     3.0      4.0   9.0           3.0   4.0  0.0     38.0       15.0    0.0\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = ['wclass', 'mstatus', 'occ', 'relationship', 'race', 'sex', 'country', 'education', 'class']\n",
    "categorical_values_train = np.array(df_train[categorical_columns])\n",
    "categorical_values_test = np.array(df_test[categorical_columns])\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(categorical_values_train[:,0]) \n",
    "data_train = le.transform(categorical_values_train[:,0])\n",
    "data_test = le.transform(categorical_values_test[:,0])\n",
    "for i in range(1, categorical_values_train.shape[1]):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(categorical_values_train[:,i])\n",
    "    data_train = np.column_stack((data_train, le.transform(categorical_values_train[:,i])))\n",
    "    data_test = np.column_stack((data_test, le.transform(categorical_values_test[:,i])))\n",
    "\n",
    "df_train_new = pd.DataFrame(data_train.astype(float), columns=categorical_columns)\n",
    "df_test_new = pd.DataFrame(data_test.astype(float), columns=categorical_columns)\n",
    "print df_train_new.head()\n",
    "print df_test_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of these, 'Education' and 'class' are features which don't require farther transformation (since it makes sense to preserve the ordinality of education, and class is something which is binray valued). For the rest, we will use onehotencoder to transform them to a non ordinal form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32561, 91)\n",
      "(16281, 91)\n",
      "Index([u'wclass_0', u'wclass_1', u'wclass_2', u'wclass_3', u'wclass_4',\n",
      "       u'wclass_5', u'wclass_6', u'wclass_7', u'mstatus_0', u'mstatus_1',\n",
      "       u'mstatus_2', u'mstatus_3', u'mstatus_4', u'mstatus_5', u'mstatus_6',\n",
      "       u'occ_0', u'occ_1', u'occ_2', u'occ_3', u'occ_4', u'occ_5', u'occ_6',\n",
      "       u'occ_7', u'occ_8', u'occ_9', u'occ_10', u'occ_11', u'occ_12',\n",
      "       u'occ_13', u'relationship_0', u'relationship_1', u'relationship_2',\n",
      "       u'relationship_3', u'relationship_4', u'relationship_5', u'race_0',\n",
      "       u'race_1', u'race_2', u'race_3', u'race_4', u'sex_0', u'sex_1',\n",
      "       u'country_0', u'country_1', u'country_2', u'country_3', u'country_4',\n",
      "       u'country_5', u'country_6', u'country_7', u'country_8', u'country_9',\n",
      "       u'country_10', u'country_11', u'country_12', u'country_13',\n",
      "       u'country_14', u'country_15', u'country_16', u'country_17',\n",
      "       u'country_18', u'country_19', u'country_20', u'country_21',\n",
      "       u'country_22', u'country_23', u'country_24', u'country_25',\n",
      "       u'country_26', u'country_27', u'country_28', u'country_29',\n",
      "       u'country_30', u'country_31', u'country_32', u'country_33',\n",
      "       u'country_34', u'country_35', u'country_36', u'country_37',\n",
      "       u'country_38', u'country_39', u'country_40', u'education', u'class',\n",
      "       u'age', u'fnlwgt', u'education-num', u'capgain', u'caploss',\n",
      "       u'hperweek'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "non_ordinal_columns = ['wclass', 'mstatus', 'occ', 'relationship', 'race', 'sex', 'country']\n",
    "non_ordinal_values_train = np.array(df_train_new[non_ordinal_columns])\n",
    "non_ordinal_values_test = np.array(df_test_new[non_ordinal_columns])\n",
    "ordinal_columns = ['education', 'class']\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(non_ordinal_values_train)\n",
    "\n",
    "data_train = enc.transform(non_ordinal_values_train)\n",
    "data_test = enc.transform(non_ordinal_values_test)\n",
    "\n",
    "cols = [non_ordinal_columns[i] + '_' + str(j) for i in range(0,len(non_ordinal_columns)) for j in range(0,enc.n_values_[i]) ]\n",
    "df_train_new2 = pd.DataFrame(data_train.toarray(),columns=cols)\n",
    "df_test_new2 = pd.DataFrame(data_test.toarray(),columns=cols)\n",
    "\n",
    "numerical_cols = ['age', 'fnlwgt', 'education-num', 'capgain', 'caploss', 'hperweek']\n",
    "df_train_cleaned = pd.concat([df_train_new2, df_train_new[ordinal_columns], df_train[numerical_cols]], axis=1)\n",
    "df_test_cleaned = pd.concat([df_test_new2, df_test_new[ordinal_columns], df_test[numerical_cols]], axis=1)\n",
    "print df_train_cleaned.shape\n",
    "print df_test_cleaned.shape\n",
    "print df_train_cleaned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Now, we normalize our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_train_cleaned.drop('class', 1)\n",
    "y_train = df_train_cleaned['class']\n",
    "x_test = df_test_cleaned.drop('class', 1)\n",
    "y_test = df_test_cleaned['class']\n",
    "scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "x_train_norm = scaler.transform(x_train)\n",
    "x_test_norm = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing one hot encoding, while providing non ordinality to our categorical variables, has really increased the dimensionality of our dataset, from 14 features to 90 features. Ideally, we should be using PCA to recuce the dimensionality of our data. \n",
    "#bla bla about how pca is not good to use here since binary discrete valued space, but we are using it anyway #because of scope of project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32561, 74)\n",
      "(16281, 74)\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=74)\n",
    "pca.fit(x_train_norm)\n",
    "'''print pca.explained_variance_ratio_\n",
    "a=pca.explained_variance_ratio_\n",
    "sum_tillnow = 0\n",
    "for i in range(90):\n",
    "    if a[i] + sum_tillnow > 0.95:\n",
    "        print i\n",
    "        break\n",
    "    else:\n",
    "        sum_tillnow = a[i] + sum_tillnow\n",
    "'''        \n",
    "x_train_transformed = pca.transform(x_train_norm)\n",
    "print x_train_transformed.shape\n",
    "x_test_transformed = pca.transform(x_test_norm)\n",
    "print x_test_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we try our hands on logistic regression. Seems to be taking a Looooooooot of time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.851237638966\n"
     ]
    }
   ],
   "source": [
    "clf = linear_model.LogisticRegression()\n",
    "kfold = KFold(n_splits=5,random_state=7)\n",
    "'''cv_results = cross_val_score(clf, x_train_transformed, y_train, cv=kfold)\n",
    "print cv_results.mean()\n",
    "predicted = cross_val_predict(clf, x_test_transformed, y_test)\n",
    "print metrics.accuracy_score(y_test, predicted) \n",
    "'''\n",
    "\n",
    "C_values = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0, 1000.0]\n",
    "param_grid = {\"C\":C_values}\n",
    "grid = GridSearchCV(estimator=clf, param_grid=param_grid, cv=kfold)\n",
    "grid.fit(x_train_transformed,y_train)\n",
    "clf2 = grid.best_estimator_\n",
    "y_pred = clf2.predict(x_test_transformed)\n",
    "print metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try a NN (might be slow to train) or SVM with gaussian kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.840606973541\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', alpha=0.01,hidden_layer_sizes=(30,), random_state=1, activation='logistic')\n",
    "cv_results = cross_val_score(clf, x_train_transformed, y_train, cv=kfold)\n",
    "print cv_results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
