{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Reddit](reddit.com) is one of the discussion websites which I visit frequently. Its a forum based website, where users can post links or text posts which can be downvoted and upvoted. One of the brilliant things about reddit is the comments - the actual \"discussion\" part of reddit. These can also be upvoted and downvoted. Topics are divided among \"subreddits\", you can submit a post to a particular subreddit, for example, a soccer based post can go to the /r/soccer subreddit, news based post to /r/news, and so on.\n",
    "\n",
    "In this blog post, I try to analyze the distribution of point counts of comments in the \"top\" subreddits and see if they have a relationship with the particular subreddit. I did take a course of stats in college, but most of that is a bit fuzzy in my head. Hence I had to end up googling a lot of things, which lead to a lot of 'reverse learning\" - where you end up learning concepts just by working on problems, and \"needing\" to use those concepts while working. This might sound like gibberish, but it'll get a bit more clearer as you progress down the post.\n",
    "\n",
    "## Getting the data\n",
    "\n",
    "To start with, I'll be getting comments distribution data for the top 1000 posts over the past year. The attributes of the post I'm interested in, are :\n",
    "\n",
    "* Comments distribution\n",
    "* Subreddit of the post\n",
    "* Submission ID (So that we can get back more information of the submission if needed)\n",
    "* score (Might be useful)\n",
    "* Number of comments\n",
    "\n",
    "[Here](https://github.com/kvsingh/reddit-scripts/blob/master/reddit-comments-top-subreddits.py) is a github link for the code to get this data, for anyone who's interested.\n",
    "\n",
    "## Subreddit breakdown\n",
    "\n",
    "Lets start by looking at the subreddits which these posts belong to, and the frequency of each subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subreddit            num_subs\n",
      "r/pics               17341453    283\n",
      "r/funny              17711708    190\n",
      "r/gifs               14865217    154\n",
      "r/aww                15740709     97\n",
      "r/gaming             16688421     49\n",
      "r/worldnews          17365378     39\n",
      "r/todayilearned      17502144     29\n",
      "r/videos             16606967     23\n",
      "r/news               14838769     21\n",
      "r/mildlyinteresting  12982562     19\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('reddit-top-1000-post-comments-distr.csv')\n",
    "print df.groupby(['subreddit', 'num_subs']).size().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/r/pics, /r/funny, /r/gifs, /r/aww, anyone who uses reddit even a decent bit will tell you its no surprise that these subreddits occupy the top spots in the \"top\" posts. Lets look at the tail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subreddit              num_subs\n",
      "r/SandersForPresident  213436      1\n",
      "r/InternetIsBeautiful  12351784    1\n",
      "r/KendrickLamar        56404       1\n",
      "r/LateStageCapitalism  163334      1\n",
      "r/LifeProTips          13112055    1\n",
      "r/woahdude             1296893     1\n",
      "r/UpliftingNews        12117045    1\n",
      "r/OldSchoolCool        11980329    1\n",
      "r/PrequelMemes         280343      1\n",
      "r/nottheonion          12311419    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print df.groupby(['subreddit', 'num_subs']).size().sort_values(ascending=False).tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redditors might be familiar with some of these, /r/nottheonion, /r/woahdude, /r/LifeProTips, but more or less, these are less popular subreddits.\n",
    "\n",
    "## Data filtering\n",
    "\n",
    "Lets filter out some of the submission which are from subreddits which have a very low count of submissions in the top 1000. The rationale being, to figure out the general relationship of comments in a subreddit, it makes sense to look at those subreddits which have a considerable number of posts. Looking at a single post from a subreddit hardly makes any sense.\n",
    "\n",
    "I decided to filter out and keep only those subreddits which have more than 10 posts in the top 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.groupby(\"subreddit\").filter(lambda x: len(x) > 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing comment vote count distributions\n",
    "\n",
    "### Mean and variance\n",
    "\n",
    "To start with, mean and standard deviation (or variance) are the most obvious things to look at when analyzing a distribution. So we create a couple of more columns in our pandas dataframe denoting the mean and standard deviation of each comment vote count distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['comment_points'] = df['comment_points'].apply(lambda a:literal_eval(a))  \n",
    "df['means'] = df['comment_points'].apply(lambda a:pd.Series(a).mean())\n",
    "df['std'] = df['comment_points'].apply(lambda a:pd.Series(a).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets look at the mean and standard deviation for 2 subreddits and compare them. Lets look at /r/news and /r/pics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           means          std\n",
      "83    707.744000  1940.516254\n",
      "87    571.812000  1553.848302\n",
      "142  1020.303213  2800.076360\n",
      "208   564.312625  1860.300583\n",
      "229   737.817269  2064.548227\n",
      "        means          std\n",
      "2  382.449597  1586.172387\n",
      "3  334.892929  1204.248983\n",
      "4  327.652000  1442.347723\n",
      "6  841.641283  4297.797784\n",
      "9  172.792683  1003.191137\n"
     ]
    }
   ],
   "source": [
    "print df[df['subreddit']=='r/news'][['means', 'std']].head()\n",
    "print df[df['subreddit']=='r/pics'][['means', 'std']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice anything peculiar? The standard deviation for r/pics posts seems to be at a much higher proportion of the mean compared to that of r/news. Could this ... mean anything? (Sorry, the pun was there for the taking).\n",
    "\n",
    "###Coefficient of variation\n",
    "\n",
    "This is when I had to start googling things. So, apparently, there is a thing called [coefficient of variation](https://en.wikipedia.org/wiki/Coefficient_of_variation), which is essentially just standard deviation divided by the mean. This can be (and is) used to compare vastly different distributions. By this, I mean distributions which vary a lot in their range of values, and have different means. As a result, just looking at the standard deviation won't help, and sd needs to be looked at in context of the mean.\n",
    "\n",
    "Lets add another column to our dataframe, called 'cv', and lets see what average cv value we are getting for each subreddit. To give some more context, I have also added the number of posts of that subreddit which were used to calculate the cv (This gives a measure of how much we can actually rely on the measurement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cv</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>r/news</th>\n",
       "      <td>379.546131</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r/worldnews</th>\n",
       "      <td>424.816224</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r/videos</th>\n",
       "      <td>432.978313</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r/todayilearned</th>\n",
       "      <td>472.853732</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r/pics</th>\n",
       "      <td>524.877848</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r/gaming</th>\n",
       "      <td>539.814921</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r/gifs</th>\n",
       "      <td>550.343261</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r/funny</th>\n",
       "      <td>557.847733</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r/aww</th>\n",
       "      <td>578.707574</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r/mildlyinteresting</th>\n",
       "      <td>595.103589</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             cv  subreddit\n",
       "subreddit                                 \n",
       "r/news               379.546131         21\n",
       "r/worldnews          424.816224         39\n",
       "r/videos             432.978313         23\n",
       "r/todayilearned      472.853732         29\n",
       "r/pics               524.877848        283\n",
       "r/gaming             539.814921         49\n",
       "r/gifs               550.343261        154\n",
       "r/funny              557.847733        190\n",
       "r/aww                578.707574         97\n",
       "r/mildlyinteresting  595.103589         19"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "df['cv'] = (df['std']/df['means'])*100\n",
    "a = df.groupby('subreddit').agg({'cv': np.mean, 'subreddit':np.size})\n",
    "a.sort_values(by='cv', axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going farther down the list, the subreddits are getting more and more \"non serious\". The top half contains more \"serious\" subreddits (like, /r/news, /r/worldnews, /r/videos, /r/todayilearned), and as you go down, you see more and more \"funny non serious\" subreddits like /r/pics, /r/funny, /r/aww.\n",
    "\n",
    "Now, technically speaking, coefficient of variation (CV from now on) only makes sense for datasets measured on \"ratio scale\". According to the wikipedia page:\n",
    "\n",
    "> The coefficient of variation should be computed only for data measured on a ratio scale, as these are the measurements that can only take non-negative values. The coefficient of variation may not have any meaning for data on an interval scale.\n",
    "\n",
    "The comment vote count distribution, is, technically not a ratio scale, since comments can have negative values. Hence , even though using CV as a metric to measure relative dispersion of distributions gives us some intuitive meaning, it is technically not a correct way to do things (As in it won't have any weight in a research paper). Therefore, I decided to probe a little farther and look at other metrics for measuring dispersion. \n",
    "\n",
    "### Other metrics\n",
    "\n",
    "Other metrics I ended up using are :\n",
    "\n",
    "* [Relative mean absolute difference (RMD)](https://en.wikipedia.org/wiki/Mean_absolute_difference#Relative_mean_absolute_difference)\n",
    "* [Quartile coefficient of dispersion](https://en.wikipedia.org/wiki/Quartile_coefficient_of_dispersion)\n",
    "\n",
    "These are metrics for measuring relative dispersion, which are defined on any kind of distribution, not just distributions from ratio scales. Quick introduction to both of them:\n",
    "\n",
    "RMD is essentially MD divided by the mean, where MD is defined as the average absolute difference between two values in the distribution. Quartile coefficient of dispersion is :\n",
    "\n",
    "> (Q3 - Q1)/(Q3 + Q1)\n",
    "\n",
    "where Q3 and Q4 are the first and third quartiles of the dataset.\n",
    "\n",
    "Lets add 2 more columns for these 2 metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
