{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Reddit](reddit.com) is one of the discussion websites which I visit frequently. Its a forum based website, where users can post links or text posts which can be downvoted and upvoted. One of the brilliant things about reddit is the comments - the actual \"discussion\" part of reddit. These can also be upvoted and downvoted. Topics are divided among \"subreddits\", you can submit a post to a particular subreddit, for example, a soccer based post can go to the /r/soccer subreddit, news based post to /r/news, and so on.\n",
    "\n",
    "In this blog post, I try to analyze the distribution of point counts of comments in the \"top\" subreddits and see if they have a relationship with the particular subreddit.\n",
    "\n",
    "## Getting the data\n",
    "\n",
    "To start with, I'll be getting comments distribution data for the top 1000 posts over the past year. The attributes of the post I'm interested in, are :\n",
    "\n",
    "* Comments distribution\n",
    "* Subreddit of the post\n",
    "* Submission ID (So that we can get back more information of the submission if needed)\n",
    "* score (Might be useful)\n",
    "* Number of comments\n",
    "\n",
    "[Here](https://github.com/kvsingh/reddit-scripts/blob/master/reddit-comments-top-subreddits.py) is a github link for the code to get this data, for anyone who's interested.\n",
    "\n",
    "## Subreddit breakdown\n",
    "\n",
    "Lets start by looking at the subreddits which these posts belong to, and the frequency of each subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subreddit            num_subs\n",
      "r/pics               17341453    283\n",
      "r/funny              17711708    190\n",
      "r/gifs               14865217    154\n",
      "r/aww                15740709     97\n",
      "r/gaming             16688421     49\n",
      "r/worldnews          17365378     39\n",
      "r/todayilearned      17502144     29\n",
      "r/videos             16606967     23\n",
      "r/news               14838769     21\n",
      "r/mildlyinteresting  12982562     19\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('reddit-top-1000-post-comments-distr.csv')\n",
    "print df.groupby(['subreddit', 'num_subs']).size().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/r/pics, /r/funny, /r/gifs, /r/aww, anyone who uses reddit even a decent bit will tell you its no surprise that these subreddits occupy the top spots in the \"top\" posts. Lets look at the tail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subreddit              num_subs\n",
      "r/SandersForPresident  213436      1\n",
      "r/InternetIsBeautiful  12351784    1\n",
      "r/KendrickLamar        56404       1\n",
      "r/LateStageCapitalism  163334      1\n",
      "r/LifeProTips          13112055    1\n",
      "r/woahdude             1296893     1\n",
      "r/UpliftingNews        12117045    1\n",
      "r/OldSchoolCool        11980329    1\n",
      "r/PrequelMemes         280343      1\n",
      "r/nottheonion          12311419    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print df.groupby(['subreddit', 'num_subs']).size().sort_values(ascending=False).tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redditors might be familiar with some of these, /r/nottheonion, /r/woahdude, /r/LifeProTips, but more or less, these are less popular subreddits.\n",
    "\n",
    "## Data filtering\n",
    "\n",
    "Lets filter out some of the submission which are from subreddits which have a very low count of submissions in the top 1000. The rationale being, to figure out the general relationship of comments in a subreddit, it makes sense to look at those subreddits which have a considerable number of posts. Looking at a single post from a subreddit hardly makes any sense.\n",
    "\n",
    "I decided to filter out and keep only those subreddits which have >=10 posts in the top 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.groupby(\"subreddit\").filter(lambda x: len(x) >= 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing comment vote count distributions\n",
    "\n",
    "To start with, mean and standard deviation (or variance) are the most obvious things to look at when analyzing a distribution. So we create a couple of more columns in our pandas dataframe denoting the mean and standard deviation of each comment vote count distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comment_points'] = df['comment_points'].apply(lambda a:literal_eval(a))  \n",
    "df['means'] = df['comment_points'].apply(lambda a:pd.Series(a).mean())\n",
    "df['std'] = df['comment_points'].apply(lambda a:pd.Series(a).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets look at the mean and standard deviation for 2 subreddits and compare them. Lets look at /r/news and /r/pics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           means          std\n",
      "83    707.744000  1940.516254\n",
      "87    571.812000  1553.848302\n",
      "142  1020.303213  2800.076360\n",
      "208   564.312625  1860.300583\n",
      "229   737.817269  2064.548227\n",
      "        means          std\n",
      "2  382.449597  1586.172387\n",
      "3  334.892929  1204.248983\n",
      "4  327.652000  1442.347723\n",
      "6  841.641283  4297.797784\n",
      "9  172.792683  1003.191137\n"
     ]
    }
   ],
   "source": [
    "print df[df['subreddit']=='r/news'][['means', 'std']].head()\n",
    "print df[df['subreddit']=='r/pics'][['means', 'std']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice anything peculiar? The standard deviation for r/pics posts seems to be at a much higher proportion of the mean compared to that of r/news. Could this ... mean anything? (Sorry, the pun was there for the taking).\n",
    "\n",
    "I went back to googling for this, since my last intersection with any sort of theoretical statistics was ... back in college. So, apparently, there is a thing called [coefficient of variation](https://en.wikipedia.org/wiki/Coefficient_of_variation), which is essentially just standard deviation divided by the mean. This can be (and is) used to compare vastly different distributions. By this, I mean distributions which vary a lot in their range of values. As a result, just looking at the standard deviation won't help, and sd needs to be looked at in context of the mean.\n",
    "\n",
    "Now, technically speaking, coefficient of variation (CV from now on) only makes sense for datasets measured on \"ratio scale\". "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
