{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[/r/soccer](https://www.reddit.com/r/soccer/) is my most frequented subreddit. I don't have stats to back that up so you'll have to take my word for it. In general, football subreddits and online forums are a gold mine for analyzing behaviour of fans. One could think of a subreddit as a \"superorganism\", having its own set of internally conflicting opinions and ideas. This is the motivation behind this blog post - analyzing fan behaviour by looking at comments on reddit's subreddits. I consider 5 subreddits (forums) - /r/soccer (the subreddit frequented by fans of all teams), /r/reddevils, /r/liverpoolfc, /r/chelseafc and /r/gunners (corresponding to fan subreddits for Manchester United, Liverpool, Chelsea and Arsenal).\n",
    "\n",
    "## Getting the Data\n",
    "\n",
    "Thanks to [python's reddit API wrapper](https://praw.readthedocs.io/en/latest/), getting comments data from reddit is easier than ever. [Here](https://github.com/kvsingh/soccer-comments/blob/master/get_data.py) is the link to the code for getting the comments using praw, and writing it to a pickle object.\n",
    "\n",
    "Let's find out some basic information about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soccer : 39005 comments\n",
      "liverpoolfc : 7363 comments\n",
      "Gunners : 6462 comments\n",
      "reddevils : 6606 comments\n",
      "chelseafc : 4617 comments\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "subreddits = [\"soccer\", \"liverpoolfc\", \"Gunners\", \"reddevils\", \"chelseafc\"]\n",
    "all_comments = {}\n",
    "for subreddit in subreddits:\n",
    "    all_comments[subreddit] = pickle.load(open(\"reddit-top-1000-post-comments-\" + subreddit + \".p\", \"rb\"))\n",
    "    print subreddit, \":\", len(all_comments[subreddit]), \"comments\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Comments on reddit are full of weird types of symbols and unicode characters. I spent some time trying to figure out the common characters/words which are irrelevant to our analysis. Any kind of text analysis will involve some customized study of our domain to figure out what characters/words we need to remove.\n",
    "\n",
    "First, we need to remove unicode characters (internet forums are full of these). In addition, reddit stores \"[removed]\" as the comment for deleted comments. We also remove \"\\n\" (newline character) and \"'s\" (I got the idea to remove this particular suffix by going through the code of [WordCloud](https://github.com/amueller/word_cloud) module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soccer : 38994 comments\n",
      "liverpoolfc : 7333 comments\n",
      "Gunners : 6400 comments\n",
      "reddevils : 6604 comments\n",
      "chelseafc : 4601 comments\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "comments_modified = {}\n",
    "for subreddit in subreddits:\n",
    "    this_comments_modified = []\n",
    "    for comment in all_comments[subreddit]:\n",
    "        comment_mod = unicodedata.normalize('NFKD', comment[1]).encode('ascii','ignore')\n",
    "        comment_mod = re.sub(r\"\\n\", \"\", comment_mod)\n",
    "        comment_mod = re.sub(r\"'s\", \"\", comment_mod)\n",
    "        if comment_mod != \"[removed]\":\n",
    "            this_comments_modified.append([comment_mod, comment[0]])\n",
    "    comments_modified[subreddit] = this_comments_modified\n",
    "    print subreddit, \":\", len(comments_modified[subreddit]), \"comments\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As an example, the following comment:\n",
    "\n",
    "> Palace fan here\\n. Well done. \\U0001f44f\\U0001f3fc\\U0001f44f\\U0001f3fc\n",
    "\n",
    "Gets modified to :\n",
    "\n",
    "> Palace fan here. Well done.\n",
    "\n",
    "For any sort of analysis, we will need to \"tokenize\" the comments, i.e., identify the important words in each comment, and transform them into a form which can be used by your classfiers/analyzers. This varies according to the domain you are working on. For the purpose of our use case, we will be performing the following steps:\n",
    "\n",
    "* Tokenize words\n",
    "* Lemmatization, or identifying the root of the word\n",
    "* Eliminating stop words and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comment_tokens = {}\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "for subreddit in subreddits:\n",
    "    comments = comments_modified[subreddit]\n",
    "    this_tokens = []\n",
    "    for comment in comments:\n",
    "        tokens = word_tokenize(comment[0])\n",
    "        tokens = [t.lower() for t in tokens]\n",
    "        tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens]\n",
    "        tokens = filter(lambda a: a not in stopwords.words('english'), tokens)\n",
    "        tokens = [re.sub(r'[^\\w\\s]','',s) for s in tokens]\n",
    "        tokens = filter(lambda a: len(a) > 2, tokens)\n",
    "        this_tokens.append(tokens)        \n",
    "    comment_tokens[subreddit] = this_tokens\n",
    "pickle.dump(comment_tokens, open(\"comment_tokens.p\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
